{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Privacy Final Project\n",
    "\n",
    "Jordan Bourdeau, Casey Forey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_10000\\3232557608.py:30: DtypeWarning: Columns (33,35,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df: pd.DataFrame = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded\n"
     ]
    }
   ],
   "source": [
    "url: str = 'https://jbourde2.w3.uvm.edu/data-privacy/data.zip'\n",
    "file_path: str = 'data/powerlifting-data.csv'\n",
    "\n",
    "# If the .zip file doesn't already exist, download it from the Silk server.\n",
    "if not os.path.exists('data.zip'):\n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        print('Downloading zip file from server')\n",
    "        open('data.zip', 'wb').write(r.content)\n",
    "        print('Zip file downloaded from server')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Unable to download zip file from remote server')\n",
    "        exit(1)\n",
    "\n",
    "# If the data folder doesn't already exist, unzip the data zip\n",
    "if not os.path.exists('data/'):\n",
    "    try:\n",
    "        with zipfile.ZipFile('data.zip') as zip:\n",
    "            zip.extractall()\n",
    "        print('Zip file extracted')\n",
    "        df = pd.read_csv(file_path)\n",
    "        print('Data read in')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('No zip file to extract from')\n",
    "        exit(1)\n",
    "\n",
    "print('Loading dataframe')\n",
    "df: pd.DataFrame = pd.read_csv(file_path)\n",
    "# Drop unneeded columns\n",
    "df = df.drop(['BirthYearClass', 'Division', 'AgeClass', 'Dots', 'Wilks', 'Glossbrenner', 'Goodlift', \n",
    "                'Federation', 'MeetCountry', 'MeetState', 'MeetTown', 'WeightClassKg',\n",
    "                'Squat4Kg', 'Bench4Kg', 'Deadlift4Kg',], axis=1)\n",
    "print('Dataframe loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho: float = 1.0\n",
    "rho_i: float = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Privacy Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit of privacy conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As is, the unit of privacy is person-meet-division-age division\n",
    "\n",
    "# 1. Remove all records which are not from a full-power division\n",
    "# Drop any rows with NaN values in it.\n",
    "# Fill NaN tested rows first\n",
    "df['Tested'] = df['Tested'].fillna(0)\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Limit to 1 record per meet (based on meet name/date for a person)\n",
    "person_meet_columns: list[str] = ['Name', 'MeetName', 'Date']\n",
    "df = df.drop_duplicates(subset=person_meet_columns, keep='first')\n",
    "\n",
    "# 3. Convert person-meet unit of privacy to person-year\n",
    "# Note: Person-state would protect a person while they reside in a specific state.\n",
    "# Identifying in terms of how we determine a 'unique' person, all data can be identifying\n",
    "# Full name + sex + competes in powerlifting is incredibly identifying so this is a proxy for a 'person'\n",
    "df['Year'] = df['Date'].map(lambda x: int(x[:4]))\n",
    "identifying_columns: list[str] = ['Name', 'Sex', 'Year']\n",
    "histogram = df.groupby(identifying_columns).size()\n",
    "noisy_histogram = gaussian_mech_zCDP_vec(histogram, 1, rho/2)\n",
    "df = df.drop(['Name'], axis=1)\n",
    "\n",
    "# This is our scalar value to divide rho by (max number of times a person has competed in a given year)\n",
    "noisy_max: float = np.max(noisy_histogram)\n",
    "rho_i = rho / (2 * noisy_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sex and tested columns into binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Mx columns for a simplifying assumption\n",
    "df = df[df['Sex'] != 'Mx']\n",
    "\n",
    "# Convert binary categorical columns into binary values\n",
    "sex: dict = {'M': 1,'F': 0}\n",
    "df['Tested'] = df['Tested'].map(lambda x: 1 if x == 'Yes' else x)\n",
    "df['Sex'] = df['Sex'].map(sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert attempts into attempt weight and success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an attempt was missed, it has a '-' put in front of it\n",
    "# Separate each attempt into the weight loaded and whether it was successful.\n",
    "attempt_columns: list[str] = ['Squat1Kg', 'Squat2Kg', 'Squat3Kg',\n",
    "                              'Bench1Kg', 'Bench2Kg', 'Bench3Kg',\n",
    "                              'Deadlift1Kg', 'Deadlift2Kg', 'Deadlift3Kg']\n",
    "\n",
    "for column in attempt_columns:\n",
    "    df[f\"{column}Made\"] = df[column].map(lambda x: 1 if x > 0 else 0)\n",
    "    df[column] = np.abs(df[column])\n",
    "\n",
    "best_attempt_columns: list[str] = ['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg']\n",
    "\n",
    "# If someone didn't hit any lifts, convert their best 3rd to 0\n",
    "for column in best_attempt_columns:\n",
    "    df[column] = df[column].map(lambda x: x if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create One-Hot-Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encodings if they don't exist\n",
    "categorical_columns: list[str] = ['Event', 'Equipment', 'ParentFederation']\n",
    "if 'encoded_features' not in locals():\n",
    "    # Create the One-Hot-Encoding\n",
    "    encoded_features: list[pd.DataFrame] = [df[column].str.get_dummies(\"|\") for column in categorical_columns if column in df.columns]\n",
    "\n",
    "# Drop the categorical columns if they are in the dataframe\n",
    "df = df.drop(categorical_columns, axis=1, errors='ignore')\n",
    "\n",
    "# Concatenate one-hot-encoded columns along the column axis\n",
    "for features in encoded_features:\n",
    "    for column in features.columns:\n",
    "        df[column] = features[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the remaining unneeded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Machine learning functions (loss, gradient, noisy gradient descent).\n",
    "'''\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta, X_test, y_test):\n",
    "    return np.sum(predict(X_test, theta) == y_test) / X_test.shape[0]\n",
    "\n",
    "# L2 Clipping\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "def noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho, learning_rate):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "    rho_count = 0.05 * rho\n",
    "    rho_i = 0.95 * rho / iterations\n",
    "    noisy_count = gaussian_mech_zCDP_vec([X_train.shape[0]], noisy_max, rho_count)[0]\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_zCDP_vec(clipped_gradient_sum, b, rho_i))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient * learning_rate\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations: int = 20\n",
    "learning_rate: float = 0.5\n",
    "num_models: int = 3\n",
    "rho_i /= num_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we predict whether someone is tested vs. untested based on their best lifts and weight?\n",
      "Scikit-learn reference:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806108897742364\n",
      "Noisy gradient descent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_10000\\1100864379.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return - (yi*xi) / (1+np.exp(exponent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806108897742364\n"
     ]
    }
   ],
   "source": [
    "# 1. Can we predict whether someone is tested vs. untested based on their best lifts and bodyweight?\n",
    "print(\"Can we predict whether someone is tested vs. untested based on their best lifts and weight?\")\n",
    "\n",
    "y = df['Tested'].values\n",
    "X = df[['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'BodyweightKg']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Scikit-learn reference:\")\n",
    "model: LogisticRegression = LogisticRegression().fit(X_train, y_train)\n",
    "print(np.sum(model.predict(X_test) == y_test) / X_test.shape[0])\n",
    "\n",
    "print(\"Noisy gradient descent:\")\n",
    "theta = noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho_i, learning_rate)\n",
    "print(accuracy(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we predict whether someone is tested vs. untested based on their equipment?\n",
      "Scikit-learn reference:\n",
      "0.8090305444887118\n",
      "Noisy gradient descent:\n",
      "0.7806108897742364\n"
     ]
    }
   ],
   "source": [
    "# 2. Can we predict whether someone competes in a tested vs. untested based on the equipment they use\n",
    "print(\"Can we predict whether someone is tested vs. untested based on their equipment?\")\n",
    "\n",
    "equipment_columns: list[str] = ['Multi-ply', 'Raw', 'Single-ply', 'Straps', 'Unlimited', 'Wraps']\n",
    "y = df['Tested'].values\n",
    "X = df[equipment_columns].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Scikit-learn reference:\")\n",
    "model: LogisticRegression = LogisticRegression().fit(X_train, y_train)\n",
    "print(np.sum(model.predict(X_test) == y_test) / X_test.shape[0])\n",
    "\n",
    "print(\"Noisy gradient descent:\")\n",
    "theta = noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho_i, learning_rate)\n",
    "print(accuracy(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we predict whether someone will hit their 3rd deadlift based on their previous lifts and other metrics?\n",
      "Scikit-learn reference:\n",
      "0.6616998671978752\n",
      "Noisy gradient descent:\n",
      "0.6404249667994688\n"
     ]
    }
   ],
   "source": [
    "# 3. Can we predict whether someone competes in a tested vs. untested based on the equipment they use?\n",
    "print(\"Can we predict whether someone will hit their 3rd deadlift based on their previous lifts and other metrics?\")\n",
    "\n",
    "predictive_columns: list[str] = ['Squat1KgMade', 'Squat2KgMade', 'Squat3KgMade', \n",
    "                                'Bench1KgMade', 'Bench2KgMade', 'Bench3KgMade', \n",
    "                                'Deadlift1KgMade', 'Deadlift2KgMade']\n",
    "y = df['Deadlift3KgMade'].values\n",
    "X = df[predictive_columns].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Scikit-learn reference:\")\n",
    "model: LogisticRegression = LogisticRegression().fit(X_train, y_train)\n",
    "print(np.sum(model.predict(X_test) == y_test) / X_test.shape[0])\n",
    "\n",
    "print(\"Noisy gradient descent:\")\n",
    "theta = noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho_i, learning_rate)\n",
    "print(accuracy(theta, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
