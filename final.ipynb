{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Privacy Final Project\n",
    "# Jordan Bourdeau, Casey Forey\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_12312\\277005906.py:31: DtypeWarning: Columns (33,35,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df: pd.DataFrame = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded\n"
     ]
    }
   ],
   "source": [
    "url: str = 'https://jbourde2.w3.uvm.edu/data-privacy/data.zip'\n",
    "file_path: str = 'data/powerlifting-data.csv'\n",
    "\n",
    "# If the .zip file doesn't already exist, download it from the Silk server.\n",
    "if not os.path.exists('data.zip'):\n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        print('Downloading zip file from server')\n",
    "        open('data.zip', 'wb').write(r.content)\n",
    "        print('Zip file downloaded from server')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Unable to download zip file from remote server')\n",
    "        exit(1)\n",
    "\n",
    "# If the data folder doesn't already exist, unzip the data zip\n",
    "if not os.path.exists('data/'):\n",
    "    try:\n",
    "        with zipfile.ZipFile('data.zip') as zip:\n",
    "            zip.extractall()\n",
    "        print('Zip file extracted')\n",
    "        df = pd.read_csv(file_path)\n",
    "        print('Data read in')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('No zip file to extract from')\n",
    "        exit(1)\n",
    "\n",
    "if 'df' not in locals():\n",
    "    print('Loading dataframe')\n",
    "    df: pd.DataFrame = pd.read_csv(file_path)\n",
    "    original: pd.DataFrame = df\n",
    "    # Drop unneeded columns\n",
    "    df = df.drop(['AgeClass', 'BirthYearClass', 'Country', 'Dots', 'Wilks', 'Glossbrenner', 'Goodlift', 'Federation', 'MeetCountry', 'MeetState', 'MeetTown', 'MeetName', 'State', 'WeightClassKg'], axis=1)\n",
    "    print('Dataframe loaded')\n",
    "else:\n",
    "    print('Dataframe already loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling/Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'Sex', 'Age', 'Division', 'BodyweightKg', 'Squat1Kg',\n",
      "       'Squat2Kg', 'Squat3Kg', 'Squat4Kg', 'Best3SquatKg', 'Bench1Kg',\n",
      "       'Bench2Kg', 'Bench3Kg', 'Bench4Kg', 'Best3BenchKg', 'Deadlift1Kg',\n",
      "       'Deadlift2Kg', 'Deadlift3Kg', 'Deadlift4Kg', 'Best3DeadliftKg',\n",
      "       'TotalKg', 'Place', 'Tested', 'Date', 'B', 'BD', 'D', 'S', 'SB', 'SBD',\n",
      "       'SD', 'Multi-ply', 'Raw', 'Single-ply', 'Straps', 'Unlimited', 'Wraps',\n",
      "       'GPA', 'GPC', 'IBSA', 'IPA', 'IPF', 'IPL', 'IRP', 'MM', 'RAW', 'SPF',\n",
      "       'UPC', 'WABDL', 'WDFPF', 'WP', 'WPA', 'WPC', 'WPF', 'WPSF', 'WPU',\n",
      "       'WRPF', 'WUAP', 'XPC'],\n",
      "      dtype='object')\n",
      "                     Name  Sex   Age Division  BodyweightKg  Squat1Kg  \\\n",
      "0             Alona Vladi  NaN  33.0        O         58.30      75.0   \n",
      "1      Galina Solovyanova  NaN  43.0       M1         73.10      95.0   \n",
      "2          Daniil Voronin  NaN  15.5        T         67.40      85.0   \n",
      "3          Aleksey Krasov  NaN  35.0        O         66.65     125.0   \n",
      "4  Margarita Pleschenkova  NaN  26.5        O         72.45      80.0   \n",
      "\n",
      "   Squat2Kg  Squat3Kg  Squat4Kg  Best3SquatKg  ...  WDFPF  WP  WPA  WPC  WPF  \\\n",
      "0      80.0     -90.0       NaN          80.0  ...      0   0    0    0    0   \n",
      "1     100.0     105.0       NaN         105.0  ...      0   0    0    0    0   \n",
      "2      90.0     100.0       NaN         100.0  ...      0   0    0    0    0   \n",
      "3     132.0     137.5       NaN         137.5  ...      0   0    0    0    0   \n",
      "4      85.0      90.0       NaN          90.0  ...      0   0    0    0    0   \n",
      "\n",
      "   WPSF  WPU  WRPF  WUAP  XPC  \n",
      "0     0    0     0     0    0  \n",
      "1     0    0     0     0    0  \n",
      "2     0    0     0     0    0  \n",
      "3     0    0     0     0    0  \n",
      "4     0    0     0     0    0  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop Mx columns for a simplifying assumption\n",
    "df = df[df['Sex'] != 'Mx']\n",
    "\n",
    "# Fill empty values with 0 for untested\n",
    "df['Tested'] = df['Tested'].fillna(0)\n",
    "\n",
    "# Convert binary categorical columns into binary values\n",
    "sex: dict = {'M': 1,'F': 0}\n",
    "tested: dict = {'Yes': 1}\n",
    "df['Tested'] = df['Tested'].map(tested)\n",
    "df['Sex'] = df['Sex'].map(sex)\n",
    "\n",
    "# Create one-hot encodings if they don't exist\n",
    "categorical_columns: list[str] = ['Event', 'Equipment', 'ParentFederation']\n",
    "if 'encoded_features' not in locals():\n",
    "    # Create the One-Hot-Encoding\n",
    "    encoded_features: list[pd.DataFrame] = [df[column].str.get_dummies(\"|\") for column in categorical_columns if column in df.columns]\n",
    "\n",
    "# Drop the categorical columns if they are in the dataframe\n",
    "df = df.drop(categorical_columns, axis=1, errors='ignore')\n",
    "\n",
    "# Concatenate one-hot-encoded columns along the column axis\n",
    "for features in encoded_features:\n",
    "    for column in features.columns:\n",
    "        df[column] = features[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Differentially private mechanisms.\n",
    "'''\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "\n",
    "''' \n",
    "Machine learning functions (loss, gradient, noisy gradient descent).\n",
    "'''\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta, X_test, y_test):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "# L2 Clipping\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "        \n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "'''\n",
    "Noisy gradient descent algorithms.\n",
    "'''\n",
    "    \n",
    "# Noisy gradient descent\n",
    "# Satisfies (k*epsilon + epsilon, k*delta)-differential privacy\n",
    "def noisy_gradient_descent(X_train, y_train, iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_vec(clipped_gradient_sum, b, epsilon, delta))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "def noisy_gradient_descent_RDP(X_train, y_train, iterations, alpha, epsilon_bar):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "    epsilon_bar_count = 0.05 * epsilon_bar\n",
    "    epsilon_bar_i = 0.95 * epsilon_bar / iterations\n",
    "    \n",
    "    noisy_count = gaussian_mech_RDP_vec([len(X_train)], 1, alpha, epsilon_bar_count)[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_RDP_vec(clipped_gradient_sum, b, alpha, epsilon_bar_i))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "def noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "    rho_count = 0.05 * rho\n",
    "    rho_i = 0.95 * rho / iterations\n",
    "  \n",
    "    noisy_count = gaussian_mech_zCDP_vec([len(X_train)], 1, rho_count)[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_zCDP_vec(clipped_gradient_sum, b, rho_i))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive regression questions to answer:\n",
    "\n",
    "* Sex\n",
    "* Age\n",
    "* Successful 3rd attempt\n",
    "* Tested vs. untested\n",
    "* Federation\n",
    "* Equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Yes\n",
      "1          Yes\n",
      "2          Yes\n",
      "3          Yes\n",
      "4          Yes\n",
      "          ... \n",
      "3023919    Yes\n",
      "3023920    Yes\n",
      "3023921    Yes\n",
      "3023922    Yes\n",
      "3023923    Yes\n",
      "Name: Tested, Length: 3023924, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Tested'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
