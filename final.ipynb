{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Privacy Final Project\n",
    "\n",
    "Jordan Bourdeau, Casey Forey\n",
    "\n",
    "Note: We used a pretty big dataset so it takes quite a while to be downloaded and extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url: str = 'https://jbourde2.w3.uvm.edu/data-privacy/data.zip'\n",
    "file_path: str = 'data/powerlifting-data.csv'\n",
    "\n",
    "# If the .zip file doesn't already exist, download it from the Silk server.\n",
    "if not os.path.exists('data.zip'):\n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        print('Downloading zip file from server')\n",
    "        open('data.zip', 'wb').write(r.content)\n",
    "        print('Zip file downloaded from server')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Unable to download zip file from remote server')\n",
    "        raise(e)\n",
    "\n",
    "# If the data folder doesn't already exist, unzip the data zip\n",
    "if not os.path.exists('data/'):\n",
    "    try:\n",
    "        with zipfile.ZipFile('data.zip') as zip_file:\n",
    "            zip_file.extractall()\n",
    "        print('Zip file extracted')\n",
    "        df = pd.read_csv(file_path)\n",
    "        print('Data read in')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('No zip file to extract from')\n",
    "        raise(e)\n",
    "\n",
    "print('Loading dataframe')\n",
    "df: pd.DataFrame = pd.read_csv(file_path)\n",
    "# Drop unneeded columns\n",
    "df = df.drop(['BirthYearClass', 'Division', 'AgeClass', 'Dots', 'Wilks', 'Glossbrenner', 'Goodlift', \n",
    "                'Federation', 'MeetCountry', 'MeetState', 'MeetTown', 'WeightClassKg',\n",
    "                'Squat4Kg', 'Bench4Kg', 'Deadlift4Kg',], axis=1)\n",
    "print('Dataframe loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho: float = .1\n",
    "max_user_contributions: int = 10\n",
    "rho_i: float = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Privacy Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit of privacy conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As is, the unit of privacy is person-meet-division-age division\n",
    "\n",
    "# 1. Remove all records which are not from a full-power division\n",
    "# Drop any rows with NaN values in it.\n",
    "# Fill NaN tested rows first\n",
    "df['Tested'] = df['Tested'].fillna(0)\n",
    "df = df[df['Event'] == 'SBD']\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Limit to 1 record per meet (based on meet name/date for a person)\n",
    "person_meet_columns: list[str] = ['Name', 'MeetName', 'Date']\n",
    "df = df.drop_duplicates(subset=person_meet_columns, keep='first')\n",
    "\n",
    "# 3. Convert person-meet unit of privacy to person with bounded number of user contributions\n",
    "df = df.groupby(['Name', 'Sex']).head(max_user_contributions)\n",
    "\n",
    "# Reset the index to remove the groupby index\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop(['Name'], axis=1)\n",
    "\n",
    "# This is our scalar value to divide rho by (max number of times a person has competed in a given year)\n",
    "rho_i = rho / (2 * max_user_contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sex and tested columns into binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Mx columns for a simplifying assumption\n",
    "df = df[df['Sex'] != 'Mx']\n",
    "\n",
    "# Convert binary categorical columns into binary values\n",
    "sex: dict = {'M': 1,'F': 0}\n",
    "df['Tested'] = df['Tested'].map(lambda x: 1 if x == 'Yes' else x)\n",
    "df['Sex'] = df['Sex'].map(sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert attempts into attempt weight and success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an attempt was missed, it has a '-' put in front of it\n",
    "# Separate each attempt into the weight loaded and whether it was successful.\n",
    "attempt_columns: list[str] = ['Squat1Kg', 'Squat2Kg', 'Squat3Kg',\n",
    "                              'Bench1Kg', 'Bench2Kg', 'Bench3Kg',\n",
    "                              'Deadlift1Kg', 'Deadlift2Kg', 'Deadlift3Kg']\n",
    "\n",
    "for column in attempt_columns:\n",
    "    df[f\"{column}Made\"] = df[column].map(lambda x: 1 if x > 0 else 0)\n",
    "    df[column] = np.abs(df[column])\n",
    "\n",
    "best_attempt_columns: list[str] = ['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg']\n",
    "\n",
    "# If someone didn't hit any lifts, convert their best 3rd to 0\n",
    "for column in best_attempt_columns:\n",
    "    df[column] = df[column].map(lambda x: x if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create One-Hot-Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encodings if they don't exist\n",
    "categorical_columns: list[str] = ['Equipment', 'ParentFederation']\n",
    "if 'encoded_features' not in locals():\n",
    "    # Create the One-Hot-Encoding\n",
    "    encoded_features: list[pd.DataFrame] = [df[column].str.get_dummies(\"|\") for column in categorical_columns if column in df.columns]\n",
    "\n",
    "# Drop the categorical columns if they are in the dataframe\n",
    "df = df.drop(categorical_columns, axis=1, errors='ignore')\n",
    "\n",
    "# Concatenate one-hot-encoded columns along the column axis\n",
    "for features in encoded_features:\n",
    "    for column in features.columns:\n",
    "        df[column] = features[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the remaining unneeded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Machine learning functions (loss, gradient, noisy gradient descent).\n",
    "'''\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta, X_test, y_test):\n",
    "    return np.sum(predict(X_test, theta) == y_test) / X_test.shape[0]\n",
    "\n",
    "# L2 Clipping\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "def noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho, learning_rate):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "    rho_count = 0.05 * rho\n",
    "    rho_i = 0.95 * rho / iterations\n",
    "    noisy_count = gaussian_mech_zCDP_vec([X_train.shape[0]], max_user_contributions, rho_count)[0]\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_zCDP_vec(clipped_gradient_sum, b, rho_i))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient * learning_rate\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations: int = 10\n",
    "learning_rate: float = 0.5\n",
    "num_models: int = 2\n",
    "rho_i /= num_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Can we predict whether someone is competing in a tested vs. untested meet based on their best lifts and weight?\")\n",
    "y = df['Tested'].values\n",
    "X = df[['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'BodyweightKg', 'Multi-ply', 'Raw', 'Single-ply', 'Unlimited', 'Wraps']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "print(f\"Percentage of tested meets: \\n{len(df[df['Tested'] == 1]) / len(df):.3f}%\")\n",
    "\n",
    "print(\"Scikit-learn reference:\")\n",
    "model: LogisticRegression = LogisticRegression().fit(X_train, y_train)\n",
    "print(np.sum(model.predict(X_test) == y_test) / X_test.shape[0])\n",
    "\n",
    "print(\"Noisy gradient descent:\")\n",
    "theta = noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho_i, learning_rate)\n",
    "print(accuracy(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Can we predict whether someone will hit their 3rd deadlift based on their previous lifts and other metrics?\")\n",
    "predictive_columns: list[str] = ['Squat1KgMade', 'Squat2KgMade', 'Squat3KgMade', \n",
    "                                'Bench1KgMade', 'Bench2KgMade', 'Bench3KgMade', \n",
    "                                'Deadlift1KgMade', 'Deadlift2KgMade']\n",
    "y = df['Deadlift3KgMade'].values\n",
    "X = df[predictive_columns].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "print(f\"Percentage of people who made their third deadlifts: \\n{len(df[df['Deadlift3KgMade'] == 1]) / len(df):.3f}%\")\n",
    "\n",
    "print(\"Scikit-learn reference:\")\n",
    "model: LogisticRegression = LogisticRegression().fit(X_train, y_train)\n",
    "print(np.sum(model.predict(X_test) == y_test) / X_test.shape[0])\n",
    "\n",
    "print(\"Noisy gradient descent:\")\n",
    "theta = noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho_i, learning_rate)\n",
    "print(accuracy(theta, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
