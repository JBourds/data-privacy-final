{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Privacy Final Project\n",
    "# Jordan Bourdeau, Casey Forey\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_10000\\1872828216.py:30: DtypeWarning: Columns (33,35,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df: pd.DataFrame = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded\n"
     ]
    }
   ],
   "source": [
    "url: str = 'https://jbourde2.w3.uvm.edu/data-privacy/data.zip'\n",
    "file_path: str = 'data/powerlifting-data.csv'\n",
    "\n",
    "# If the .zip file doesn't already exist, download it from the Silk server.\n",
    "if not os.path.exists('data.zip'):\n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        print('Downloading zip file from server')\n",
    "        open('data.zip', 'wb').write(r.content)\n",
    "        print('Zip file downloaded from server')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Unable to download zip file from remote server')\n",
    "        exit(1)\n",
    "\n",
    "# If the data folder doesn't already exist, unzip the data zip\n",
    "if not os.path.exists('data/'):\n",
    "    try:\n",
    "        with zipfile.ZipFile('data.zip') as zip:\n",
    "            zip.extractall()\n",
    "        print('Zip file extracted')\n",
    "        df = pd.read_csv(file_path)\n",
    "        print('Data read in')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('No zip file to extract from')\n",
    "        exit(1)\n",
    "\n",
    "print('Loading dataframe')\n",
    "df: pd.DataFrame = pd.read_csv(file_path)\n",
    "# Drop unneeded columns\n",
    "df = df.drop(['BirthYearClass', 'Division', 'AgeClass', 'Dots', 'Wilks', 'Glossbrenner', 'Goodlift', \n",
    "                'Federation', 'MeetCountry', 'MeetState', 'MeetTown', 'WeightClassKg',\n",
    "                'Squat4Kg', 'Bench4Kg', 'Deadlift4Kg',], axis=1)\n",
    "print('Dataframe loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho: float = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Differentially private mechanisms.\n",
    "'''\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling/Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.293952244043492\n"
     ]
    }
   ],
   "source": [
    "# As is, the unit of privacy is person-meet-division-age division\n",
    "\n",
    "# 1. Remove all records which are not from a full-power division\n",
    "# Drop any rows with NaN values in it.\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Limit to 1 record per meet (based on meet name/date for a person)\n",
    "person_meet_columns: list[str] = ['Name', 'MeetName', 'Date']\n",
    "df = df.drop_duplicates(subset=person_meet_columns, keep='first')\n",
    "\n",
    "# 3. Convert person-meet unit of privacy to person-year\n",
    "# Note: Person-state would protect a person while they reside in a specific state.\n",
    "# Identifying in terms of how we determine a 'unique' person, all data can be identifying\n",
    "# Full name + sex + competes in powerlifting is incredibly identifying so this is a proxy for a 'person'\n",
    "df['Year'] = df['Date'].map(lambda x: int(x[:4]))\n",
    "identifying_columns: list[str] = ['Name', 'Sex', 'Year']\n",
    "histogram = df.groupby(identifying_columns).size()\n",
    "noisy_histogram = gaussian_mech_zCDP_vec(histogram, 1, rho/2)\n",
    "\n",
    "# This is our scalar value to divide rho by (max number of times a person has competed in a given year)\n",
    "noisy_max: float = np.max(noisy_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Mx columns for a simplifying assumption\n",
    "df = df[df['Sex'] != 'Mx']\n",
    "\n",
    "# Fill empty values with 0 for untested\n",
    "df['Tested'] = df['Tested'].fillna(0)\n",
    "\n",
    "# Convert binary categorical columns into binary values\n",
    "sex: dict = {'M': 1,'F': 0}\n",
    "tested: dict = {'Yes': 1}\n",
    "df['Tested'] = df['Tested'].map(tested)\n",
    "df['Sex'] = df['Sex'].map(sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each attempt into the attempt weight and another column for whether they made it or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an attempt was missed, it has a '-' put in front of it\n",
    "# Separate each attempt into the weight loaded and whether it was successful.\n",
    "attempt_columns: list[str] = ['Squat1Kg', 'Squat2Kg', 'Squat3Kg',\n",
    "                              'Bench1Kg', 'Bench2Kg', 'Bench3Kg',\n",
    "                              'Deadlift1Kg', 'Deadlift2Kg', 'Deadlift3Kg']\n",
    "\n",
    "for column in attempt_columns:\n",
    "    df[f\"{column}Made\"] = df[column].map(lambda x: 1 if x > 0 else 0)\n",
    "    df[column] = np.abs(df[column])\n",
    "\n",
    "best_attempt_columns: list[str] = ['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg']\n",
    "\n",
    "# If someone didn't hit any lifts, convert their best 3rd to 0\n",
    "for column in best_attempt_columns:\n",
    "    df[column] = df[column].map(lambda x: x if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encodings if they don't exist\n",
    "categorical_columns: list[str] = ['Event', 'Equipment', 'ParentFederation']\n",
    "if 'encoded_features' not in locals():\n",
    "    # Create the One-Hot-Encoding\n",
    "    encoded_features: list[pd.DataFrame] = [df[column].str.get_dummies(\"|\") for column in categorical_columns if column in df.columns]\n",
    "\n",
    "# Drop the categorical columns if they are in the dataframe\n",
    "df = df.drop(categorical_columns, axis=1, errors='ignore')\n",
    "\n",
    "# Concatenate one-hot-encoded columns along the column axis\n",
    "for features in encoded_features:\n",
    "    for column in features.columns:\n",
    "        df[column] = features[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Machine learning functions (loss, gradient, noisy gradient descent).\n",
    "'''\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta, X_test, y_test):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "# L2 Clipping\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "        \n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "'''\n",
    "Noisy gradient descent algorithms.\n",
    "'''\n",
    "    \n",
    "# Noisy gradient descent\n",
    "# Satisfies (k*epsilon + epsilon, k*delta)-differential privacy\n",
    "def noisy_gradient_descent(X_train, y_train, iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_vec(clipped_gradient_sum, b, epsilon, delta))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "def noisy_gradient_descent_RDP(X_train, y_train, iterations, alpha, epsilon_bar):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "    epsilon_bar_count = 0.05 * epsilon_bar\n",
    "    epsilon_bar_i = 0.95 * epsilon_bar / iterations\n",
    "    \n",
    "    noisy_count = gaussian_mech_RDP_vec([len(X_train)], 1, alpha, epsilon_bar_count)[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_RDP_vec(clipped_gradient_sum, b, alpha, epsilon_bar_i))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "def noisy_gradient_descent_zCDP(X_train, y_train, iterations, rho):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    b = 3\n",
    "    rho_count = 0.05 * rho\n",
    "    rho_i = 0.95 * rho / iterations\n",
    "  \n",
    "    noisy_count = gaussian_mech_zCDP_vec([len(X_train)], 1, rho_count)[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_zCDP_vec(clipped_gradient_sum, b, rho_i))\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive regression questions to answer:\n",
    "\n",
    "* Sex\n",
    "* Age\n",
    "* Successful 3rd attempt\n",
    "* Tested vs. untested\n",
    "* Federation\n",
    "* Equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
